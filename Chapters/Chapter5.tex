% Chapter 5

\chapter{Concept Blending Algorithm} % Main chapter title

\label{Chapter5} % For referencing the chapter elsewhere, use \ref{Chapter5} 

%----------------------------------------------------------------------------------------

\section{Concept blending}
My implementation of a concept blending algorithm consists of three steps. The first is retrieving the input data consisting of concept descriptions from a data source such as WikiPedia. The second step is creating the generic space, the structure deriving from both inputs. This is done by retrieving generalizations of the nouns from each concept, creating a tree structure of shared hypernyms. The third step is finding suggestions of elements to blend by using a heuristic function to rank the elements using patterns based on characteristics we have found in good blending examples.

%----------------------------------------------------------------------------------------

\section{Preparing input data}

\subsection{Finding useful properties}
In our approach to concept blending, the elements that make the input spaces are nouns from the WikiPedia description. The reason for this is that we can generalize the nouns by retrieving their hypernyms using WordNet. The reason we focus on nouns is that they are more likely elements of the concept rather than verbs, adjectives or any other part-of-speech. Verbs are actions, occurrences or state of beings, rather than elements. Although verbs can be generalized with hypernyms as well, we can not compare the generality of verbs and nouns, since verbs usually have fewer hypernyms than nouns. The characteristics we look for are based on the distances in the hypernym tree. The distances we look for are the number of hypernyms between the word, the least general shared hypernym and the most general shared hypernym. If the compatibility score of the blending suggestions is based on how specialized the word is, it will favour nouns over verbs, since in WordNet verbs usually have fewer hypernyms than nouns, and it would not make sense to compare verbs and nouns this way.

An optimal solution may retrieve the properties and all the relations between the words to create a more complete representation of the concept, and not just elements. We have only focused on properties alone since we want to validate the use of generalization in conceptual blending. For further research on a more complete algorithm for conceptual blending, it may be possible to further elaborate the representation, keeping the original relations between the elements from the input spaces to the blend space. It could also be possible to generalize the relations using WordNet, but research should be done on how these relations will be scored and blended.

\subsection{An example}

Lets say we have two concepts that we want to blend, for example Cajón and Clapsticks.
We can retrieve the abstracts using a HTTP call to the wikipedia API:

\noindent\fbox{
	\parbox{\textwidth}{
		\url{http://en.wikipedia.org/w/api.php?action=parse&page=conceptName&prop=text&section=0&format=json&redirects=1&callback=?}
	}
}
\\\\The parameter ConceptName stands for the name of our concept. By extracting the first paragraph element (<p>) in the returned JSON string, we can retrieve the raw abstract. For Cajón this abstract is:
 
\noindent\fbox{
	\parbox{\textwidth}{
		A cajón is nominally a box-shaped percussion instrument originally from Peru, played by slapping the front or rear faces with the hands, fingers, or sometimes various implements such as brushes, mallets, or sticks.
	}
}
\\\\In order for the algorithm to be able to provide suggestions for the best properties to transfer between two concepts, we need to find properties in abstract that may represent the concept. To be properly able to query WordNet with words from the text we need to process the abstract with the steps seen in \textbf{procedure 1}. Initially the descriptions consists of a lot of common words we are not interested in called stop-words. We use a stop-word list to remove them. But before they are removed, we send the original abstract to the Stanford part-of-speech (POS) tagger to have each word tagged. The POS tagger takes each word in a sentence and use a heuristic method to find out which part-of-speech the word is, such as noun, verb or adjective. This data is useful when we want to reduce the number of potential word meanings in the disambiguation step. We will only get the synsets of that part-of-speech. If the word \emph{bear} is a noun, we only get the nouns such as the animal, not the verb as in \emph{bear a resemblance}. Since it is a heuristic method, the POS tagger may not be correct, in which case we get only the synsets of the wrong pos. The reason we have to tag the words so early in the algorithm is because the POS tagger needs the whole text in order to provide the best suggestions. If we remove stop-words, we lose words that may be important for estimating the correct part-of-speech. 

The abstract of cajón after processing and removal of stop-words and duplicate words:\\

\noindent\fbox{
	\parbox{\textwidth}{
		cajón nominally box-shaped percussion instrument originally peru played slapping front rear faces hands fingers sometimes various implements brushes mallets sticks
	}
}
%First the algorithm tries to grammatically tag each word using the Stanford part-of-speech tagger.

\begin{algorithm}
	\caption{Process a WikiPedia abstract to prepare words for WordNet queries}\label{euclid}
	\begin{algorithmic}[1]
		\Require wikipediaAbstract,
		\Ensure processedWikipediaAbstract, wikipediaAbstractWithoutStopwords
		\Procedure{processWikipediaAbstract}{}
		\For {\textit{wikipediaAbstract}}
		\State Remove extra newline
		\State Replace line breaks with whitespaces
		\State Split words separated by forward slash
		\State Remove \&\#160; to avoid the number 160 appearing in the text
		\State Remove [] and the containing text (e.g reference tags)
		\State Remove paranthesis and the containing text
		\State Remove forward-slash and the containing text (e.g phonetics)
		\State Remove - at the end of words
		\State Remove d' at the start of french words
		\State Remove l' at the start of french words
		\EndFor
		\State $\textit{wikipediaAbstractWithoutStopwords} \gets \textit{wikipediaAbstract}$
		\For {\textit{wikipediaAbstractWithoutStopwords}}
		\State Remove 's at the end of words
		\State Remove all apostrophes
		%\State Remove symbols \texttt{[!@#$%^&*(){}?<>+:;",.♭♯£€¤=\`_~}
		\State Remove symbols $!@\#\$\%\&*(){}?<>+\hat{}:;",.'\flat\sharp=\textbackslash\grave{}\_\tilde{}\pounds$\texteuro\textcurrency
		\State Remove words that has numbers in it
		\State Change to lower case (WordNet requires lowercase string)
		
		\State Remove stop-words
		\State Remove duplicate words
		\EndFor
		
		\State \Return [\textit{wikipediaAbstract},  \textit{wikipediaAbstractWithoutStopwords}]
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

%----------------------------------------------------------------------------------------

\subsection{Identifying noun phrases}
When analysing a text for useful properties, some of them may be described by multiple words or a noun phrase. Take for example the two-worded property \emph{sustaining pedal}, which is one of the pedals on a piano that lifts the dampers from the strings to let them continue vibrating. The word sustaining and pedal can not sufficiently describe the sustaining pedal on their own. The meaning of the word sustaining depends on the word pedal, and it may not be clear if the word pedal is a sustaining pedal, a soft pedal or any other pedal unrelated to pianos. Therefore identifying noun phrases can lead to finding more useful properties in the text. We can identify noun phrases by using the Stanford parser which takes a sentence and label the noun phrases with a NP tag. We can then remove unnecessary parts of the noun phrase such as determinants. If the noun phrase contains more than two words, we can recursively check the tail for other noun phrases. For example in the noun phrase \emph{electric five-string bass} there is also the noun phrase \emph{five-string bass}, and \emph{bass}.

%----------------------------------------------------------------------------------------

\subsection{Synsets}
WordNet provides a set of synsets for each word in the database, and each of these synsets has a unique definition. If we search for the word \emph{string}, WordNet returns a set of 10 nouns and 7 verbs. We want to search WordNet for every word we found in the abstract and find the correct synsets so that we have a list of unambiguous properties representing the concept. WordNet also store the POS of a word, so we can use the POS tagger data to remove every synset of the wrong POS. For instance if we have tagged \emph{piano} as a noun, we can remove the adjective \emph{piano} which means soft.
%Nevne tagger her? Kan luke vekk nouns eller verbs

\subsection{Stemming}
Sometimes WordNet gives no results because the word is in a different variant than the one in the WordNet database. For instance if the word is \emph{guitars}, it does not match \emph{guitar} in the database. To return \emph{guitar}, we can use a stemming algorithm to find the stem of the word. Often the stem is found in WordNet. This method is not always perfect though. Sometimes the stemmer can over-stem, cutting too much of the word leading to a word that has a different meaning. %Example?
%Pos tagging can avoid this.
%Vurdere lemmatizer istaden då dei gir bedre meining? Updates = updat i stemmer. Lemmatizer bruker wordnet

%----------------------------------------------------------------------------------------

\subsection{Word sense disambiguation}
Now that we have the key words and noun phrases extracted from the abstract, we will try to find the correct meaning of each word. Let's say we want to find the synset corresponding to the \emph{bow} used to play the \emph{cello}. How can we do this? We have to get a list of words that we will look for in each definition of \emph{bow}. We call this list \emph{scoring words}. \emph{Scoring words} will consist of each word in the processed abstract plus a set of musical context words that can act as the musical context if the abstract lack sufficient musical words. Most likely the definitions we are looking for are related to a musical context. We use the algorithm in \textbf{procedure 2} shown below. It is based on the \emph{Simple Lesk} algorithm.
\\The \emph{context words} we use in this example are:\\
\noindent\fbox{
	\parbox{\textwidth}{music, instrument, tone, tones, sounds, sounding, rhythm, melody, drum, flute, percussion, idiophone, bass, baritone, tenor, alto, soprano, note, pitch, audio, auditory, tune, tuning, acoustic}
}
\\\\The \emph{scoring words} combines these with the words of the \emph{cello} abstract:

\noindent\fbox{
	\parbox{\textwidth}{
		cello, violoncello, bowed, plucked, string, instrument, four, strings, tuned, perfect, fifths, low, octave, lower, viola, violin, family, musical, instruments, includes, double, bass, solo, chamber, music, ensembles, orchestras, section, symphony, types, rock, bands, second-largest, lowest, modern, orchestra, largest, pitch
	}
}
\\\\We will remove duplicates of words that are in both lists so that each word is equally influential on the scoring. Duplicate words are:

\noindent\fbox{
	\parbox{\textwidth}{music, instrument, bass, pitch}
}
\\\\To add score to a definition, we count the number of words (excluding stop-words) in each synset definition which also exists in the \emph{scoring words}. The first definition of \emph{bow} is:

\noindent\fbox{\parbox{\textwidth}{
		a knot with two loops and loose ends; used to tie shoelaces
}}
\\\\None of these words are in the \emph{scoring words} of a cello, therefore we give it a score of 0. The next definition is:\\

\noindent\fbox{\parbox{\textwidth}{
		a slightly curved piece of resilient wood with taut horsehair strands; used in playing certain stringed instruments
}}
\\\\One word, instrument, is found in the cello scoring words, therefore we give it a score of 1. We select the synset that has the highest score, which in this case was the correct bow definition. This approach is heuristic and will not always find the correct synset.
Sometimes the word is similar to, but not equal to the scoring word. For example if the definition had the word \emph{bow}, and scoring words had \emph{bowed}, we would not find it. One way to match these would be to search for \emph{bow} in the start of the string \emph{bowed}, and also search for \emph{bowed} in the start of \emph{bow}. Now we would get matches when the scoring word starts with the definition word and vice versa. An alternative way could be to use a stemming algorithm on the words and see if either the scoring word or scoring word stem matches the definition word or definition word stem.

An alternative additional scoring system could be to use \emph{context synsets}, specific synsets that automatically give an extra point for being relevant to the category. For example the musical synset of the word \emph{instrument} would automatically get an extra point over non-musical synsets of instruments like the more general synset \emph{device}.

Other techniques could be to give more score to certain words or synsets than others. Some words may be more important, such as the concept name itself. One of the definitions of \emph{hammer} include the word \emph{piano}, which is clearly an important word if this is also the instrument. Therefore it could get a score of 5 when other scoring words get 1 point.

\begin{algorithm}
	\caption{Get the definition which is most likely to fit the word}\label{euclid}
	\begin{algorithmic}[1]
		\Require ambigousWord,
		definitions,
		scoringWords
		\Ensure highest scoring definition
		\Procedure{GetMostLikelyDefinition}{}
		\For {\textbf{each} \textit{definition} in \textit{definitions}}
		\State $\textit{definitionScore} \gets 0$
		\For {\textbf{each} \textit{definitionWord} in \textit{definition}}
		\For {\textbf{each} \textit{scoringWord} in \textit{scoringWords}}
		\If {\textit{scoringWord} matches \textit{ambigousWord}}
		\State \textbf{continue}
		\EndIf
		\If {\textit{scoringWord} is the first part of \textit{definitionWord}
		\textbf{or} \textit{definitionWord} is the first part of \textit{scoringWord}}
		\State ${\textit{definitionScore} \gets {\textit{definitionScore}+1}}$
		\EndIf
		\EndFor
		\EndFor
		\If {$\textit{definitionScore} > \textit{definitionMaxScore}$}
		\State ${\textit{definitionMaxScore}} \gets {\textit{definitionScore}}$
		\State ${\textit{topDefinition}} \gets {\textit{definition}}$
		\EndIf
		\EndFor
		\If {there is no \textit{topDefinition}}
		\Return null
		\EndIf
		\State \Return \textit{topDefinition}
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

%----------------------------------------------------------------------------------------

\section{Generic space}
\subsection{Generalization using hypernyms}
Our approach to concept blending is inspired by the use of amalgams. When using WordNet we can easily generalize since we have access to each synsets tree of hypernyms. So if we consider the concepts \emph{a red French vehicle} and \emph{a German minivan}, we can find that \emph{French} and \emph{German} share the hypernym \emph{nation}, and minivan and vehicle share the hypernym \emph{vehicle}.

The point of the algorithm is to find properties in one concept that has correlating properties in the other concept. If the other concept has a property in the same category, there may be a possibility for the property to fit into the other concept.

Our solution give score proportional to the speciality of the word and penalize proportionally to how general the generalization is, aiming for the most specific generalization. In the example above, minivan should get a good score, since it is specific and the other concept is a vehicle, which makes not a too general generalization. However a minivan and SUV would give a better score, since they are both cars, which is less general than vehicle.

The algorithm we use to create the generic space between is shown below as \textbf{procedure 3}. We can pass \emph{Cajón} as the source concept and \emph{Clapsticks} as the target concept. After processing the abstracts and finding the most likely synsets of each property, we can use WordNet to retrieve a list of every hypernym ancestor of each synset, which will go from the least general to the most general generalization. They can be accessed from the array \emph{concept.property.hypernyms}. To create the generic space, we simply retrieve all synsets and hypernymsynsets in each concept and use a intersection set operation to keep what we find in both concepts.

\begin{algorithm}
	\caption{Get all synsets in the generic space between two concepts}\label{euclid}
	\begin{algorithmic}[1]
		\Require sourceConcept,
		targetConcept
		\Ensure highest scoring definition
		\Procedure{GetGenericSpace}{}
		\For {\textbf{each} \textit{property} in \textit{sourceConcept.properties}}
		\State ${\textit{sourceSynsets} \gets {\textit{sourceSynsets} + \textit{property}}}$
		\For {\textbf{each} \textit{hypernym} in \textit{property.hypernyms}}
		\State ${\textit{sourceSynsets} \gets {\textit{sourceSynsets} + \textit{hypernym}}}$
		\EndFor
		\EndFor
		
		\For {\textbf{each} \textit{property} in \textit{targetConcept.properties}}
		\State ${\textit{targetSynsets} \gets {\textit{targetSynsets} + \textit{property}}}$
		\For {\textbf{each} \textit{hypernym} in \textit{property.hypernyms}}
		\State ${\textit{targetSynsets} \gets {\textit{targetSynsets} + \textit{hypernym}}}$
		\EndFor
		\EndFor
		\State ${\textit{sourceSynsets} \gets {removeDuplicates(\textit{sourceSynsets})}}$
		\State ${\textit{targetSynsets} \gets {removeDuplicates(\textit{targetSynsets})}}$
		\State \Return ${SetOperations.intersection(\textit{sourceSynsets}, \textit{targetSynsets})}$
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

\subsection{Finding matches}
In order to provide good suggestions, we need to find properties that share a hypernym in the generic space with some property in the other concept. For instance a football field and a golf terrain share a common hypernym \emph{piece of land} which means they show a kind of commonality and could therefore be more compatible for blending than less related pairs. 

%----------------------------------------------------------------------------------------

\section{Finding good blending suggestions}
We came up with a solution where properties from each concept are sorted with a score that represents how well they are suited for blending. We were improving the algorithm by identifying patterns typical of good candidates in the tree structure of the property words and their hypernyms.
\subsection{Depth of the word in hypernym tree}
The first pattern we found was that words that have high generality are not useful. We want more specific words like \emph{drumstick} rather than general and vague words like \emph{object}. Therefore we gave points equal to the depth of the word in the tree.
\subsection{Branch length to shared hypernyms}
The second pattern we found was when properties of two concepts share a common root or hypernym, the length of the branch from this root seemed to be useful information. The longer the branch length, the worse were the candidate. Therefore we punished the word by subtracting points equal to the branch length.
\subsection{Penalizing certain categories}
Now the winning words were specific, but too often abstract, being an instrument in itself or a person (e.g. the inventor). Therefore we chose to punish the words significantly when being in the branch of \emph{abstract entities}, \emph{instruments} or \emph{individuals}. The resulting suggestions were then both physical and specific, which seems to be optimal for creating new instruments.
\subsection{Removing properties in penalized categories}
An observation of the properties that connected in the general space, showed that a lot of these were in penalized categories. For example a property of the penalized category instrument is a device, which makes devices in the other concept score higher than if there were no other devices in the first concept. A way to avoid this could be to remove properties of penalized categories altogether. This leads to far less blending suggestions, but the suggestions that are left are of relatively high quality. Since no blending suggestions are in penalized categories also makes the penalty score redundant.

\subsection{Ranking Compatible Blending Suggestions}
First we find each property that are not in the generic space themselves but has a hypernym in the generic space. This means that this property can be blended and we can give it a compatibility score for whether it is a good blending suggestion. We call the branch from the synset and hypernyms subsuming its hypernyms in the generic space for \emph{blending subtree}. We set the compatibility score to the depth of its least general hypernym in the generic space minus the length of the \emph{blending subtree}. Finally we can optionally subtract a constant penalty value if the property is or has a hypernym in a penalized category. With this score we can store the unique properties in a list which we can rank using a heuristic function. The algorithm for finding unique properties and ranking them is shown below in \textbf{procedure 4}.

\begin{algorithm}
	\caption{Perform creative strategy by ranking a concepts subtrees by finding a pair in the other concept with the highest compability, defined by the expression Depth - BranchLength - Penalty}\label{euclid}
	\begin{algorithmic}[1]
		\Require properties,
		concept,
		genericSpace,
		penaltyCategories,
		penaltyPoints
		\Ensure a list of branches ranked by their blending compatibility score
		\Procedure{RankCompatibleBlendingSuggestions}{}
		\For {\textbf{each} \textit{property} in \textit{properties}}
		\If {$\textit{property.name} = \textit{concept.name}$
		\\\qquad\qquad\textbf{or} \textit{property} has not been disambiguated
		\\\qquad\qquad\textbf{or} \textit{property} is in \textit{genericSpace}}
		\State \textbf{continue}
		\EndIf
		\For {\textit{i} = 0 to \textit{property.hypernyms.length}}
		\If {\textit{property.hypernyms[i]} is not in \textit{genericSpace}}
		\State {\textbf{continue}}
		\EndIf
		\State ${\textit{subtree.branch} \gets {[\textit{property}] \text{ concatinated with } \textit{property.hypernyms[0:i]}}}$
		\State ${\textit{depth} \gets {\textit{property.hypernyms.length} - i}}$
		\State ${\textit{branchLength} \gets {\textit{subtree.branch.length}}}$
		\If {\textit{subtree.branch} has an element in \textit{penaltyCategories}}
		\State ${\textit{penalty} \gets \textit{penaltyPoints}}$
		\EndIf
		\State ${\textit{subtree.compatibility} \gets {\textit{depth} - \textit{branchLength} - \textit{penalty}}}$
		\State ${\textit{subtrees} \gets {\textit{subtrees} \text{ concatinated with } \textit{subtree}}}$
		\State {\textbf{break} inner for loop}
		\EndFor
		\EndFor
		\State \Return \textit{subtrees} \textbf{sorted} by \textit{subtree.compability} in descending order
	\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Elaboration of blend}
After ranking the blending suggestions in each concept, we are left with an open number of possibilities on how to elaborate the blend. The least radical blending would be to move the element with the highest score over to the other concept. When blending \emph{guitar} and \emph{violin}, the highest score our implementation gives to an element is the \emph{bow} in violin. Therefore we can move the bow over to guitar, and we can assume that the bow will be used on the guitar in a similar manner to how it is used on a violin. We have not represented any other relation between bow and violin than the fact that \emph{violin has bow}. And now \emph{guitar has bow}. It will require human interpretation of the original description to further understand how it is used as our solution does not do any further interpretation of the elements and their relation to the concept.

Other possibilities to how to blend can be to have a compatibility threshold, so that all elements that are given a compatibility score over this threshold are moved to the other concept. You can then look at either two blended concepts, or the concept with the highest sum of scores could be selected as the blend space. The compatibility score ranges from a negative value if the branch length is higher than the depth, to a positive value if the depth is higher than the branch length. The ideal threshold may be hard to define, it would depend on whether the number of hypernyms is a good measure of generality. In some cases in WordNet there may be many hypernyms to describe a short distance in generality. Some things may have more categories than others. For example, the chess pieces have a lot of hypernyms since they are among diverse types of object, but if the chess piece \emph{rook} were the only gaming equipment to exist, we would not need hypernyms such as \emph{chess piece} or \emph{board game piece}. Therefore we argue that some types of objects that have not diversified as much as they might do in the future, and this would imply that the number of hypernyms may not be a good measure of generality, when comparing different types of entities. But it may be enough to tell us a somewhat vague indication of the generality which could be useful even though it is not perfect. Further research should be done on how to achieve a better measure of generality.

%----------------------------------------------------------------------------------------

\section{Future work}