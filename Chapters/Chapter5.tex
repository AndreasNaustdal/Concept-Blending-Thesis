% Chapter 5

\chapter{Concept Blending Algorithm} % Main chapter title

\label{Chapter5} % For referencing the chapter elsewhere, use \ref{Chapter5} 

%----------------------------------------------------------------------------------------

\section{Concept blending}
My implementation of a concept blending algorithm consists of three steps. The first is retrieving the input data consisting of concept descriptions from a data source such as WikiPedia. The second step is creating the generic space, the structure deriving from both inputs. This is done by retrieving generalizations of the words from each concept, creating a tree structure of shared hypernyms. The third step is finding suggestions of elements to blend by using a heuristic function to rank the elements using patterns based on characteristics we have found in good blending examples.

%----------------------------------------------------------------------------------------

\section{Preparing input data}

\subsection{Finding useful properties}
We have two concepts that we want to blend, for example Cajón and Clapsticks.
We can retrieve the abstracts using a HTTP call to the wikipedia API:

\noindent\fbox{
	\parbox{\textwidth}{
		\url{http://en.wikipedia.org/w/api.php?action=parse&page=conceptName&prop=text&section=0&format=json&redirects=1&callback=?}
	}
}
The parameter ConceptName stands for the name of our concept. By extracting the first <p> paragraph element in the returned JSON string, we can retrieve the raw abstract. For Cajón this abstract is:
 
\noindent\fbox{
	\parbox{\textwidth}{
		A cajón is nominally a box-shaped percussion instrument originally from Peru, played by slapping the front or rear faces with the hands, fingers, or sometimes various implements such as brushes, mallets, or sticks.
	}
}

In order for the algorithm to be able to provide suggestions for the best properties to transfer between two concepts, we need to find properties in abstract that may represent the concept. To make this easier we need to process the abstract with the steps seen in \textbf{procedure 1}. Initially the descriptions consists of a lot of common words we are not interested in called stop-words. We use a stop-word list to remove them. But before they are removed we send the original abstract to the Stanford POS tagger to have each word tagged. This data is useful when we want to reduce the number of potential word meanings in the disambiguation step.
The abstract of cajón after processing and removal of stopwords and duplicate words:
\noindent\fbox{
	\parbox{\textwidth}{
		cajón nominally box-shaped percussion instrument originally peru played slapping front rear faces hands fingers sometimes various implements brushes mallets sticks
	}
}
%First the algorithm tries to grammatically tag each word using the Stanford part-of-speech tagger.

\begin{algorithm}
	\caption{Process a WikiPedia abstract}\label{euclid}
	\begin{algorithmic}[1]
		\Require wikipediaAbstract,
		\Ensure processedWikipediaAbstract, wikipediaAbstractWithoutStopwords
		\Procedure{processWikipediaAbstract}{}
		\For {\textit{wikipediaAbstract}}
		\State Remove extra newline
		\State Replace line breaks with whitespaces
		\State Split words separated by forward slash
		\State Remove \&\#160; to avoid the number 160 appearing in the text
		\State Remove [] and the containing text (e.g reference tags)
		\State Remove paranthesis and the containing text
		\State Remove forward-slash and the containing text (e.g phonetics)
		\State Remove - at the end of words
		\State Remove d' at the start of french words
		\State Remove l' at the start of french words
		\EndFor
		\State $\textit{wikipediaAbstractWithoutStopwords} \gets \textit{wikipediaAbstract}$
		\For {\textit{wikipediaAbstractWithoutStopwords}}
		\State Remove 's at the end of words
		\State Remove all apostrophes
		%\State Remove symbols \texttt{[!@#$%^&*(){}?<>+:;",.♭♯£€¤=\`_~}
		\State Remove symbols $!@\#\$\%\&*(){}?<>+\hat{}:;",.'\flat\sharp=\textbackslash\grave{}\_\tilde{}\pounds$\texteuro\textcurrency
		\State Remove words that has numbers in it
		\State Change to lower case (WordNet requires lowercase string)
		
		\State Remove stopwords
		\State Remove duplicate words
		\EndFor
		
		\State \Return [\textit{wikipediaAbstract},  \textit{wikipediaAbstractWithoutStopwords}]
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

%----------------------------------------------------------------------------------------

\subsection{Identifying noun phrases}
When analysing a text for useful properties, some of them may be described by multiple words or a noun phrase. Take for example the two-worded property \emph{sustaining pedal}, which is one of the pedals on a piano that lifts the dampers from the strings to let them continue vibrating. The word sustaining and pedal can not sufficiently describe the sustaining pedal on their own. The word sustaining depends on the word pedal, and it may not be clear if the word pedal is a sustaining pedal, a soft pedal or any other pedal unrelated to pianos. Therefore identifying noun phrases can lead to finding more useful properties in the text. We can identify noun phrases by using the Stanford parser which takes a sentence and label the noun phrases with a NP tag. We can then remove unnecessary parts of the noun phrase such as determinants. If the noun phrase contains more than two words, we can recursively check the tail for other noun phrases. For example in the noun phrase \emph{electric five-string bass} there is also the noun phrase \emph{five-string bass}.

%----------------------------------------------------------------------------------------

\subsection{WordNet database}
WordNet provides a set of synsets for each word in the database, and each of these synsets has a unique definition. If we search for the word \emph{string}, WordNet returns a set of 10 nouns and 7 verbs. We want to search WordNet for every word we found in the abstract and find the correct synsets so that we have a list of unambiguous properties representing the concept. WordNet also store the POS of a word, so we can use the POS tagger data to remove every synset of the wrong POS. For instance if we have tagged \emph{piano} as a noun, we can remove the adjective \emph{piano} which means soft.
%Nevne tagger her? Kan luke vekk nouns eller verbs

\subsection{Stemming}
Sometimes WordNet gives no results because the word is in a different variant than the one in the WordNet database. For instance if the word is \emph{guitars}, it does not match \emph{guitar} in the database. To return \emph{guitar}, we can use a stemming algorithm to find the stem of the word. Often the stem is found in WordNet. This method is not always perfect though. Sometimes the stemmer can over-stem, cutting too much of the word leading to a word that has a different meaning. %Example?
%Pos tagging can avoid this.
%Vurdere lemmatizer istaden då dei gir bedre meining? Updates = updat i stemmer. Lemmatizer bruker wordnet

%----------------------------------------------------------------------------------------

\subsection{Word sense disambiguation}
Let's say we want to find the synset corresponding to the bow used to play stringed instruments. How can we do this? One way is to count the number of words (excluding stop-words) in each synset definition which also exists in the concept text. The first definition of bow is \emph{a knot with two loops and loose ends; used to tie shoelaces}. None of these words are in the description of a guitar, therefore we give it a score of 0. The next definition is \emph{a slightly curved piece of resilient wood with taut horsehair strands; used in playing certain stringed instruments}. The words wood and instruments is found in the violin abstract, therefore we give it a score of 2. We select the synset that has the highest score. This approach is heuristic, it will not always find the correct synset.

To improve the accuracy in the category musical instruments, we added musical context words that also score one point when found in the definition. These words are 'music', 'instrument', 'tone', 'tones', 'sounds', 'sounding', 'rhythm', 'melody', 'drum', 'flute', 'percussion', 'idiophone',
'bass', 'baritone', 'tenor', 'alto', 'soprano', 'note', 'pitch', 'audio', 'auditory', 'tune', 'tuning' and 'acoustic'.
An alternative method could be to use context synsets instead, specific synsets that were automatically given an extra point for being relevant to the category.

For every word that has multiple possible meanings we run the following algorithm. The input is each definition that we have returned from the word search in WordNet. We remove the stopwords in the definitions to reduce irrelevant words. We also need all the words in abstract and the context words, since we want to look for these in each definition. We call these words \textit{scoring words}.

\begin{algorithm}
	\caption{Get the definition which is most likely to fit the word}\label{euclid}
	\begin{algorithmic}[1]
		\Require ambigousWord,
		definitions,
		scoringWords
		\Ensure highest scoring definition
		\Procedure{GetMostLikelyDefinition}{}
		\For {\textbf{each} \textit{definition} in \textit{definitions}}
		\State $\textit{definitionScore} \gets 0$
		\For {\textbf{each} \textit{definitionWord} in \textit{definition}}
		\For {\textbf{each} \textit{scoringWord} in \textit{scoringWords}}
		\If {\textit{scoringWord} matches \textit{ambigousWord}}
		\State \textbf{continue}
		\EndIf
		\If {\textit{scoringWord} is the first part of \textit{definitionWord}
		\textbf{or} \textit{definitionWord} is the first part of \textit{scoringWord}}
		\State ${\textit{definitionScore} \gets {\textit{definitionScore}+1}}$
		\EndIf
		\EndFor
		\EndFor
		\If {$\textit{definitionScore} > \textit{definitionMaxScore}$}
		\State ${\textit{definitionMaxScore}} \gets {\textit{definitionScore}}$
		\State ${\textit{topDefinition}} \gets {\textit{definition}}$
		\EndIf
		\EndFor
		\If {there is no \textit{topDefinition}}
		\Return null
		\EndIf
		\State \Return \textit{topDefinition}
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

%----------------------------------------------------------------------------------------

\section{Generic space}
\subsection{Generalization using hypernyms}
Our approach to concept blending is inspired by the use of amalgams. When using WordNet we can easily generalize since we have access to each synsets tree of hypernyms. So if we consider the concepts \emph{a red French vehicle} and \emph{a German minivan}, we can find that \emph{French} and \emph{German} share the hypernym \emph{nation}, and minivan and vehicle share the hypernym \emph{vehicle}.\\
Our solution give score proportional to the speciality of the word and penalize proportionally to how general the generalization is, aiming for the least general generalization. In the example above, minivan should get a good score, since it is specific and the other concept is a vehicle, which makes not a too general generalization. However a minivan and SUV would give a better score, since they are both cars, which is less general than vehicle.

\begin{algorithm}
	\caption{Get all synsets in the generic space between two concepts}\label{euclid}
	\begin{algorithmic}[1]
		\Require sourceConcept,
		targetConcept
		\Ensure highest scoring definition
		\Procedure{GetGenericSpace}{}
		\For {\textbf{each} \textit{property} in \textit{sourceConcept.properties}}
		\State ${\textit{sourceSynsets} \gets {\textit{sourceSynsets} + \textit{property}}}$
		\For {\textbf{each} \textit{hypernym} in \textit{property.hypernyms}}
		\State ${\textit{sourceSynsets} \gets {\textit{sourceSynsets} + \textit{hypernym}}}$
		\EndFor
		\EndFor
		
		\For {\textbf{each} \textit{property} in \textit{targetConcept.properties}}
		\State ${\textit{targetSynsets} \gets {\textit{targetSynsets} + \textit{property}}}$
		\For {\textbf{each} \textit{hypernym} in \textit{property.hypernyms}}
		\State ${\textit{targetSynsets} \gets {\textit{targetSynsets} + \textit{hypernym}}}$
		\EndFor
		\EndFor
		\State ${\textit{sourceSynsets} \gets {removeDuplicates(\textit{sourceSynsets})}}$
		\State ${\textit{targetSynsets} \gets {removeDuplicates(\textit{targetSynsets})}}$
		\State \Return ${SetOperations.intersection(\textit{sourceSynsets}, \textit{targetSynsets})}$
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

\subsection{Finding matches}
In order to provide good suggestions, we need to find pairs of properties that share a common hypernym. For instance a football field and a golf terrain share a common hypernym \emph{piece of land} which means they show a kind of commonality and could therefore be more compatible for blending than less related pairs. We take each subtree of shared hypernyms in the graph of the generic space and store them in a list which we can rank using a heuristic function.

\begin{algorithm}
	\caption{Perform creative strategy by ranking a concepts subtrees by finding a pair in the other concept with the highest compability, defined by the expression Depth - BranchLength - Penalty}\label{euclid}
	\begin{algorithmic}[1]
		\Require properties,
		concept,
		genericSpace,
		penaltyCategories,
		penaltyPoints
		\Ensure a list of branches ranked by their blending compatibility score
		\Procedure{RankCompatibleBlendingSuggestions}{}
		\For {\textbf{each} \textit{property} in \textit{properties}}
		\If {$\textit{property.name} = \textit{concept.name}$
		\\\qquad\qquad\textbf{or} \textit{property} has not been disambiguated
		\\\qquad\qquad\textbf{or} \textit{property} is in \textit{genericTree}}
		\State \textbf{continue}
		\EndIf
		\For {\textit{i} = 0 to \textit{property.hypernyms.length}}
		\If {\textit{property.hypernyms[i]} is not in \textit{genericSpace}}
		\State {\textbf{continue}}
		\EndIf
		\State ${\textit{subtree.branch} \gets {[\textit{property}] \text{ concatinated with } \textit{property.hypernyms[0:i]}}}$
		\State ${\textit{depth} \gets {\textit{property.hypernyms.length} - i}}$
		\State ${\textit{branchLength} \gets {\textit{subtree.branch.length}}}$
		\If {\textit{subtree.branch} has an element in \textit{penaltyCategories}}
		\State ${\textit{penalty} \gets \textit{penaltyPoints}}$
		\EndIf
		\State ${\textit{subtree.compatibility} \gets {\textit{depth} - \textit{branchLength} - \textit{penalty}}}$
		\State ${\textit{subtrees} \gets {\textit{subtrees} \text{ concatinated with } \textit{subtree}}}$
		\State {\textbf{break} inner for loop}
		\EndFor
		\EndFor
		\State \Return \textit{subtrees} \textbf{sorted} by \textit{subtree.compability} in descending order
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

%----------------------------------------------------------------------------------------

\section{Finding good blending suggestions}
We came up with a solution where properties from each concept are sorted with a score that represents how well they are suited for blending. We were improving the algorithm by identifying patterns typical of good candidates in the tree structure of the property words and their hypernyms.
\subsection{Depth of the word in hypernym tree}
The first pattern we found was that words that have high generality are not useful. We want more specific words like \emph{drumstick} rather than general and vague words like \emph{object}. Therefore we gave points equal to the depth of the word in the tree.
\subsection{Branch length to shared hypernyms}
The second pattern we found was when properties of two concepts share a common root or hypernym, the length of the branch from this root seemed to be useful information. The longer the branch length, the worse were the candidate. Therefore we punished the word by subtracting points equal to the branch length.
\subsection{Penalizing certain categories}
Now the winning words were specific, but too often abstract, being an instrument in itself or a person (e.g. the inventor). Therefore we chose to punish the words significantly when being in the branch of \emph{abstract entities}, \emph{instruments} or \emph{individuals}. The resulting suggestions were then both physical and specific, which seems to be optimal for creating new instruments.

%----------------------------------------------------------------------------------------