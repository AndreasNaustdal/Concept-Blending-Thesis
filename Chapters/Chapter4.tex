% Chapter 4

\chapter{Development} % Main chapter title

\label{Chapter4} % For referencing the chapter elsewhere, use \ref{Chapter4} 

%----------------------------------------------------------------------------------------

\section{Tools and technologies}
In this thesis we have developed an algorithm which provide suggestions of which properties that are most compatible for blending between two concepts of a domain such as musical instruments. To achieve this we have used natural language processing tools such as Stanford coreNLP and WordNet.

\subsection{WikiPedia}
WikiPedia is a large online encyclopedia, consisting of over 5 million english articles. We use it as a data source for the concept descriptions. We query WikiPedia for the concept or instrument we want to blend. Then we take the first section in the article, which we refer to as the abstract. The results depend on the quality of this abstract. Sometimes it includes irrelevant words, for example cultural references such as the inventor or where the instrument is produced.

\subsection{WordNet}
WordNet contains a large group of English words that are linked together with its synonym group called synset. Each synset has a distinct meaning, so a word that can mean different things has one synset for each meaning. This is useful for preparing the data so we do not include two elements that mean the same. WordNet also provides antonyms of a word, which is useful to detect contradictions in the blended concept.

\subsection{Stanford coreNLP toolkit}
The Stanford coreNLP toolkit is widely used for natural language analysis. It includes tools for tokenization, sentence splitting, part-of-speech tagging, morphological analysis, named entity recognition, syntactic tagging, coreference resolution and more. \parencite{manning-EtAl:2014:P14-5}

\subsection{JavaScript Frameworks and Libraries}

The implementation was written in JavaScript using the frameworks AngularJS and Node.JS. The algorithm uses the following libraries from the Node Package Manager (NPM):
\subsection{wordnet-magic}
wordnet-magic is a Node.JS implementation of Princeton's WordNet lexical database for the English language. We use it to retrieve a set of synsets for each word in the description of the concepts. When the correct synsets are chosen, we also use it to retrieve all hypernyms of the synset in a tree structure where every ancestor is stored as synsets. 

\subsection{stanford-simple-nlp}
stanford-simple-nlp is a Node.JS wrapper for StanfordCoreNLP, a set of natural language processing tools. We use two of their tools, a sentence parser and a part-of-speech tagger. We use the sentence parser to identify noun phrases when we want to find synsets of elements that are represented in multiple words. We use the part-of-speech tagger to find the part of speech (POS) of every word in the concept description, categorizing them in categories like nouns and verbs. This is used to remove synsets of the wrong POS.

%Data sources:
%WikiPedia
%My application lets you enter the name of a concept. It locates the corresponding WikiPedia article, copying its abstract which is the first paragraph of the article. This text is used as the basis for finding the properties of the concept using natural language processing tools and techniques.

%Node libraries:
%Natural Language Processing tools:
%wordnet-magic
%stanford-simple-nlp?
%includes
%Stanford Parser
%Stanford Tagger
%Pos Tagger

%Javascript libraries:
%Vis.js

%Languages:
%Javascript

%Frameworks:
%AngularJS
%Express

%Environments:

%Node.JS
%Sourcetree
%Git
%GitHub
%TeXstudio
%LaTeX
%Trello

%----------------------------------------------------------------------------------------

\section{Evolution of software}

%----------------------------------------------------------------------------------------

\section{Application structure}
A simplifed overview of the application, its components and data flow can be seen in Figure~\ref{fig:application-structure}.

\begin{figure}
\centering
\includegraphics[width=1\linewidth]{"Figures/Application structure"}
\caption{Overview of the application with the flow of data through the key methods}
\label{fig:application-structure}
\end{figure}

\subsection{Stemming algorithm}
The application send each word of the WikiPedia abstract without stopwords to the stemming algorithm to get eventual stems of the word. WordNet does not recognize all forms of the word, so by stemming we can get access to more synsets.

\subsection{WordNet}
WordNet is a machine-readable dictionary containing 155,287 unique words and 117,659 synsets. \parencite{fellbaum1998wordnet} \parencite{wordnetstatistics}
\\The application use WordNet in to ways when retrieving language data:
\begin{itemize}
\item Retrieving synsets by querying the WordNet database with words and word stems to get the synsets of each word.
\item Retrieving hypernyms by querying the WordNet database with synsets to get all hypernyms of that synset.
\end{itemize}

\subsection{POS tagger}
The application sends the whole WikiPedia abstract of each concept as a string into the POS tagger, to retrieve an two-dimensional array where the first column contain the words and the second contain the part-of-speech tag.

\subsection{Get most likely definition}
The application use the \emph{get most likely definition} method as a heuristic function to find the synset that most likely carry the correct definition of the word. The method takes the following input:
\begin{itemize}
	\item The ambigous word in question
	\item The different definitions of the word
	\item A list of scoring words, which is made up of the abstract words and a set of words relating to the context (musical instruments)
\end{itemize}


\subsection{Concept}
We have two concepts we want to blend. Each concept has the following data:
\begin{itemize}
\item A name which we will use to find the corresponding WikiPedia article for
\item A WikiPedia abstract
\item The WikiPedia abstract without stopwords
\item A list of properties.
\end{itemize}

\subsection{Wikipedia Abstract}
The WikiPedia abstract contains a text with the description of the concept.

\subsection{Properties}
Each property has the following data:
\begin{itemize}
\item The original word
\item A list containing the original word and eventual stems found by the stemming algorithm.
\item The part-of-speech such as noun, verb or adjective which have been tagged by the POS tagger
\item A list of synsets, any unique meaning that the word or the stem of the word may have. Can be pruned to store only synsets of the same part-of-speech that the POS tagger found.
\item A reference to the synset of the most likely definition found by the \emph{get most likely definition} method
\item A list of hypernyms, from the most specific to the most general, starting with the direct ancestor of the synset.
\end{itemize}

\subsection{Generic Space}
The generic space contains a list of synsets and hypernym synsets that exist in both concepts.

\subsection{Subtrees}
Each concept has a list of subtrees containing unique properties and their hypernyms that are subsuming but are not themselves in the generic space. They are given a compatibility score by the \emph{rank compatible blending suggestions} method. The subtree is assigned a depth, a branch length and a penalty value. The depth value contains its least general hypernym in the generic space.
The branch length is the number of nodes in the subtree. The penalty value is equal to a penalty constant if the property is or has a hypernym in a penalized category.

\subsection{Rank compatible blending suggestions}
Each subtree are given a compatibility score. This is calculated by the depth minus branch length minus penalty. They are then sorted by this compatibility score.

%----------------------------------------------------------------------------------------