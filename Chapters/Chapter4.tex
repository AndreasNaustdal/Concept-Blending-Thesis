% Chapter 4

\chapter{Development} % Main chapter title

\label{Chapter4} % For referencing the chapter elsewhere, use \ref{Chapter4} 

%----------------------------------------------------------------------------------------

\section{Tools and technologies}
In this thesis we have developed an algorithm which provide suggestions of which properties that are most compatible for blending between two concepts of a domain such as musical instruments. To achieve this we have used natural language processing tools such as Stanford coreNLP and WordNet.

\subsection{WikiPedia}
WikiPedia is a large online encyclopedia, consisting of over 5 million english articles. We use it as a data source for the concept descriptions, by scraping the first section or abstract of the article.

\subsection{WordNet}
WordNet contains a large group of English words that are linked together with its synonym group called synset. Each synset has a distinct meaning, so a word that can mean different things has one synset for each meaning. This is useful for preparing the data so we do not include two elements that mean the same. WordNet also provides antonyms of a word, which is useful to detect contradictions in the blended concept.

\subsection{Stanford coreNLP toolkit}
The Stanford coreNLP toolkit is widely used for natural language analysis. It includes tools for tokenization, sentence splitting, part-of-speech tagging, morphological analysis, named entity recognition, syntactic tagging, coreference resolution and more. \parencite{manning-EtAl:2014:P14-5}

\subsection{JavaScript Frameworks and Libraries}

The implementation was written in JavaScript using the frameworks AngularJS and Node.JS. The algorithm uses the following libraries from the Node Package Manager (NPM):
\subsection{wordnet-magic}
wordnet-magic is a Node.JS implementation of Princeton's WordNet lexical database for the English language. We use it to retrieve a set of synsets for each word in the description of the concepts. When the correct synsets are chosen, we also use it to retrieve all hypernyms of the synset in a tree structure where every ancestor is stored as synsets. 

\subsection{stanford-simple-nlp}
stanford-simple-nlp is a Node.JS wrapper for StanfordCoreNLP, a set of natural language processing tools. We use two of their tools, a sentence parser and a part-of-speech tagger. We use the sentence parser to identify noun phrases when we want to find synsets of elements that are represented in multiple words. We use the part-of-speech tagger to find the part of speech (POS) of every word in the concept description, categorizing them in categories like nouns and verbs. This is used to remove synsets of the wrong POS.

%Data sources:
%WikiPedia
%My application lets you enter the name of a concept. It locates the corresponding WikiPedia article, copying its abstract which is the first paragraph of the article. This text is used as the basis for finding the properties of the concept using natural language processing tools and techniques.

%Node libraries:
%Natural Language Processing tools:
%wordnet-magic
%stanford-simple-nlp?
%includes
%Stanford Parser
%Stanford Tagger
%Pos Tagger

%Javascript libraries:
%Vis.js

%Languages:
%Javascript

%Frameworks:
%AngularJS
%Express

%Environments:

%Node.JS
%Sourcetree
%Git
%GitHub
%TeXstudio
%LaTeX
%Trello

%----------------------------------------------------------------------------------------

\section{Evolution of software}

%----------------------------------------------------------------------------------------

\section{Application structure}
A simplifed overview of the application, its components and data flow can be seen in \textbf{figure 4.1} above.

\begin{figure}
\centering
\includegraphics[width=1\linewidth]{"Figures/Application structure"}
\caption{Overview of the application with the flow of data through the key methods}
\label{fig:application-structure}
\end{figure}


%----------------------------------------------------------------------------------------